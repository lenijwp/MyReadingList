# Backdoor Reading List

‚õîÔ∏è `Pending` üïê `Updating` üëç `Finished` ‚≠ê `1-5 Stars`

**Description:** xxx.

**Keywords: AI Backdoor; Neural Network Backdoor; Backdoor Attack on Neural Network; AI Trojan; Neural Trojan; Trojan Neural Network**

## Survey

1.  ‚õîÔ∏è **A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models (2020 CCS)**
    [[Notes](./notes/pang2020tale.md)]
    [[Paper](https://arxiv.org/pdf/1911.01559)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Comparing the adversarial attack and backdoor attack.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{pang2020tale,
        author    = {Ren Pang and
                    Hua Shen and
                    Xinyang Zhang and
                    Shouling Ji and
                    Yevgeniy Vorobeychik and
                    Xiapu Luo and
                    Alex Liu and
                    Ting Wang},
        title     = {A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models},
        booktitle = {Proceedings of ACM SAC Conference on Computer and Communications (CCS)},
        year      = {2020}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1911.01559
    ```

    </details>

2.  ‚õîÔ∏è **A Survey on Neural Trojans (2020 IACR)**
    [[Notes](./notes/liu2020survey.md)]
    [[Paper](https://eprint.iacr.org/2020/201.pdf)]

    **Overview:** A survey.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{liu2020survey,
        author    = {Yuntao Liu and
                    Ankit Mondal and
                    Abhishek Chakraborty and
                    Michael Zuzak and
                    Nina Jacobsen and
                    Daniel Xing and
                    Ankur Srivastava},
        title     = {A Survey on Neural Trojans},
        journal   = {{IACR} Cryptol. ePrint Arch.},
        volume    = {2020},
        pages     = {201},
        year      = {2020},
        url       = {https://eprint.iacr.org/2020/201},
        timestamp = {Mon, 11 May 2020 15:59:15 +0200},
        biburl    = {https://dblp.org/rec/journals/iacr/LiuMCZJXS20.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://eprint.iacr.org/2020/201.pdf
    Citation: https://dblp.org/rec/bibtex/journals/iacr/LiuMCZJXS20
    ```

    </details>

## Attack

1.  ‚õîÔ∏è **BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain (2020 arXiv)**
    [[Notes](./notes/gu2017badnets.md)]
    [[Paper](https://arxiv.org/pdf/1708.06733)]
    [[Code](https://github.com/Kooscii/BadNets)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Backdoor attack for transfer learning, using data poisoning.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{gu2017badnets,
        author    = {Tianyu Gu and
                    Brendan Dolan{-}Gavitt and
                    Siddharth Garg},
        title     = {BadNets: Identifying Vulnerabilities in the Machine Learning Model
                    Supply Chain},
        journal   = {CoRR},
        volume    = {abs/1708.06733},
        year      = {2017},
        url       = {http://arxiv.org/abs/1708.06733},
        archivePrefix = {arXiv},
        eprint    = {1708.06733},
        timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1708-06733.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1708.06733
    Code: https://github.com/Kooscii/BadNets
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1708-06733
    ```

    </details>

2.  ‚õîÔ∏è **Neural Trojans (2017 ICCD)**
    [[Notes](./notes/liu2017trojans.md)]
    [[Paper](https://arxiv.org/pdf/1710.00942)]

    **Overview:** Backdoor attack based on data poisoning.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{liu2017trojans,
        author    = {Yuntao Liu and
                    Yang Xie and
                    Ankur Srivastava},
        title     = {Neural Trojans},
        booktitle = {2017 {IEEE} International Conference on Computer Design, {ICCD} 2017,
                    Boston, MA, USA, November 5-8, 2017},
        pages     = {45--48},
        publisher = {{IEEE} Computer Society},
        year      = {2017},
        url       = {https://doi.org/10.1109/ICCD.2017.16},
        doi       = {10.1109/ICCD.2017.16},
        timestamp = {Mon, 15 Jun 2020 17:06:54 +0200},
        biburl    = {https://dblp.org/rec/conf/iccd/LiuXS17.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1710.00942
    Citation: https://dblp.org/rec/bibtex/conf/iccd/LiuXS17
    ```

    </details>

3.  ‚õîÔ∏è **Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning (2017 arXiv)**
    [[Notes](./notes/chen2017target.md)]
    [[Paper](https://arxiv.org/pdf/1712.05526)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{chen2017target,
        author    = {Xinyun Chen and
                    Chang Liu and
                    Bo Li and
                    Kimberly Lu and
                    Dawn Song},
        title     = {Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning},
        journal   = {CoRR},
        volume    = {abs/1712.05526},
        year      = {2017},
        url       = {http://arxiv.org/abs/1712.05526},
        archivePrefix = {arXiv},
        eprint    = {1712.05526},
        timestamp = {Mon, 22 Jul 2019 13:37:30 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1712-05526.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1712.05526
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1712-05526
    ```

    </details>

4.  ‚õîÔ∏è **Trojaning Attack on Neural Networks (2018 NDSS)**
    [[Notes](./notes/liu2018trojannn.md)]
    [[Paper](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)]
    ‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** Backdoor attack for pre-trained models, using data poisoning.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{liu2018trojannn,
        author    = {Yingqi Liu and
                    Shiqing Ma and
                    Yousra Aafer and
                    Wen{-}Chuan Lee and
                    Juan Zhai and
                    Weihang Wang ands
                    Xiangyu Zhang},
        title     = {Trojaning Attack on Neural Networks},
        booktitle = {25th Annual Network and Distributed System Security Symposium, {NDSS}
                    2018, San Diego, California, USA, February 18-21, 2018},
        publisher = {The Internet Society},
        year      = {2018},
        url       = {http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018\_03A-5\_Liu\_paper.pdf},
        timestamp = {Thu, 09 Aug 2018 10:57:16 +0200},
        biburl    = {https://dblp.org/rec/conf/ndss/LiuMALZW018.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }

    ```

    #### URL

    ```
    Paper: https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech
    Citation: https://dblp.org/rec/bibtex/conf/ndss/LiuMALZW018
    ```

    </details>

5.  ‚õîÔ∏è **SIN<sup>2</sup>: Stealth Infection on Neural Network - A Low-cost Agile Neural Trojan Attack Methodology (2018 HOST)**
    [[Notes](./notes/liu2018sin2.md)]
    [[Paper](http://jin.ece.ufl.edu/papers/HOST18_DNN.pdf)]

    **Overview:** Binary-level backdoor attack.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{liu2018sin2,
        author    = {Tao Liu and
                    Wujie Wen and
                    Yier Jin},
        title     = {SIN\({}^{\mbox{2}}\): Stealth Infection on Neural Network - {A} Low-cost
                    Agile Neural Trojan Attack Methodology},
        booktitle = {2018 {IEEE} International Symposium on Hardware Oriented Security
                    and Trust, {HOST} 2018, Washington, DC, USA, April 30 - May 4, 2018},
        pages     = {227--230},
        publisher = {{IEEE} Computer Society},
        year      = {2018},
        url       = {https://doi.org/10.1109/HST.2018.8383920},
        doi       = {10.1109/HST.2018.8383920},
        timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
        biburl    = {https://dblp.org/rec/conf/host/LiuWJ18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: http://jin.ece.ufl.edu/papers/HOST18_DNN.pdf
    Citation: https://dblp.org/rec/bibtex/conf/host/LiuWJ18
    ```

    </details>

6.  ‚õîÔ∏è **Hu-Fu: Hardware and Software Collaborative Attack Framework Against Neural Networks (2018 ISVLSI)**
    [[Notes](./notes/li2018hufu.md)]
    [[Paper](https://arxiv.org/pdf/1805.05098)]

    **Overview:** A hardware-software collaborative attack framework to inject hidden neural network trojans.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{li2018hufu,
        author    = {Wenshuo Li and
                    Jincheng Yu and
                    Xuefei Ning and
                    Pengjun Wang and
                    Qi Wei and
                    Yu Wang and
                    Huazhong Yang},
        title     = {Hu-Fu: Hardware and Software Collaborative Attack Framework Against
                    Neural Networks},
        booktitle = {2018 {IEEE} Computer Society Annual Symposium on VLSI, {ISVLSI} 2018,
                    Hong Kong, China, July 8-11, 2018},
        pages     = {482--487},
        publisher = {{IEEE} Computer Society},
        year      = {2018},
        url       = {https://doi.org/10.1109/ISVLSI.2018.00093},
        doi       = {10.1109/ISVLSI.2018.00093},
        timestamp = {Wed, 16 Oct 2019 14:14:54 +0200},
        biburl    = {https://dblp.org/rec/conf/isvlsi/LiYNWWWY18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1805.05098
    Citation: https://dblp.org/rec/bibtex/conf/isvlsi/LiYNWWWY18
    ```

    </details>

7.  ‚õîÔ∏è **Latent Backdoor Attacks on Deep Neural Networks (2019 CCS)**
    [[Notes](./notes/yao2019latent.md)]
    [[Paper](http://people.cs.uchicago.edu/~huiyingli/publication/fr292-yaoA.pdf)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Attack for transfer learning scenario, the backdoor behaviors can be persisted after transfer learning.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{yao2019latent,
        author    = {Yuanshun Yao and
                    Huiying Li and
                    Haitao Zheng and
                    Ben Y. Zhao},
        editor    = {Lorenzo Cavallaro and
                    Johannes Kinder and
                    XiaoFeng Wang and
                    Jonathan Katz},
        title     = {Latent Backdoor Attacks on Deep Neural Networks},
        booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} Conference on Computer and
                    Communications Security, {CCS} 2019, London, UK, November 11-15, 2019},
        pages     = {2041--2055},
        publisher = {{ACM}},
        year      = {2019},
        url       = {https://doi.org/10.1145/3319535.3354209},
        doi       = {10.1145/3319535.3354209},
        timestamp = {Mon, 11 May 2020 18:04:22 +0200},
        biburl    = {https://dblp.org/rec/conf/ccs/YaoLZZ19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: http://people.cs.uchicago.edu/~huiyingli/publication/fr292-yaoA.pdf
    Citation: https://dblp.org/rec/bibtex/conf/ccs/YaoLZZ19
    ```

    </details>

8.  ‚õîÔ∏è **A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning (2018 ICIP)**
    [[Notes](./notes/barni2019corruption.md)]
    [[Paper](https://arxiv.org/pdf/1902.11237)]

    **Overview:** Backdoor attacks without label poisoning.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{barni2019corruption,
        author    = {Mauro Barni and
                    Kassem Kallas and
                    Benedetta Tondi},
        title     = {A New Backdoor Attack in {CNNS} by Training Set Corruption Without
                    Label Poisoning},
        booktitle = {2019 {IEEE} International Conference on Image Processing, {ICIP} 2019,
                    Taipei, Taiwan, September 22-25, 2019},
        pages     = {101--105},
        publisher = {{IEEE}},
        year      = {2019},
        url       = {https://doi.org/10.1109/ICIP.2019.8802997},
        doi       = {10.1109/ICIP.2019.8802997},
        timestamp = {Wed, 11 Dec 2019 16:30:23 +0100},
        biburl    = {https://dblp.org/rec/conf/icip/BarniKT19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1902.11237
    Citation: https://dblp.org/rec/bibtex/conf/icip/BarniKT19
    ```

    </details>

9.  ‚õîÔ∏è **TrojanNet: Embedding Hidden Trojan Horse Models in Neural Networks (2020 arXiv)**
    [[Notes](./notes/guo2020trojannet.md)]
    [[Paper](https://arxiv.org/pdf/2002.10078)]

    **Overview:** A public and secret task can be simultaneously learned in a single network.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{guo2020trojannet,
        author    = {Chuan Guo and
                    Ruihan Wu and
                    Kilian Q. Weinberger},
        title     = {TrojanNet: Embedding Hidden Trojan Horse Models in Neural Networks},
        journal   = {CoRR},
        volume    = {abs/2002.10078},
        year      = {2020},
        url       = {https://arxiv.org/abs/2002.10078},
        archivePrefix = {arXiv},
        eprint    = {2002.10078},
        timestamp = {Tue, 03 Mar 2020 14:32:13 +0100},
        biburl    = {https://dblp.org/rec/journals/corr/abs-2002-10078.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/2002.10078
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-2002-10078
    ```

    </details>

10. ‚õîÔ∏è **Graph Backdoor (2020 arXiv)**
    [[Notes](./notes/xi2020graphbackdoor.md)]
    [[Paper](https://arxiv.org/pdf/2006.11890)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Backdoor attack for graph neural network.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{xi2020graphbackdoor,
        author    = {Zhaohan Xi and
                    Ren Pang and
                    Shouling Ji and
                    Ting Wang},
        title     = {Graph Backdoor},
        journal   = {CoRR},
        volume    = {abs/2006.11890},
        year      = {2020},
        url       = {https://arxiv.org/abs/2006.11890},
        archivePrefix = {arXiv},
        eprint    = {2006.11890},
        timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-2006-11890.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/2006.11890
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-2006-11890
    ```

    </details>

11. üëç **The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models (2020 arXiv)**
    [[Notes](./notes/karra2020trojai.md)]
    [[Paper](https://arxiv.org/pdf/2003.07233)]
    [[Code](https://github.com/trojai/trojai)]
    [[Doc](https://trojai.readthedocs.io/en/latest/)]
    ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** An open source framework for generating triggered datasets and corresponding trojaned models using `datagen`, `modelgen` and `pipeline` modules.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{karra2020trojai,
        author    = {Kiran Karra and
                    Chace Ashcraft and
                    Neil Fendley},
        title     = {The TrojAI Software Framework: An OpenSource tool for Embedding Trojans
                    into Deep Learning Models},
        journal   = {CoRR},
        volume    = {abs/2003.07233},
        year      = {2020},
        url       = {https://arxiv.org/abs/2003.07233},
        archivePrefix = {arXiv},
        eprint    = {2003.07233},
        timestamp = {Tue, 17 Mar 2020 14:18:27 +0100},
        biburl    = {https://dblp.org/rec/journals/corr/abs-2003-07233.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/2003.07233
    Code: https://github.com/trojai/trojai
    Documentation: https://trojai.readthedocs.io/en/latest/
    Competition: https://pages.nist.gov/trojai/
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-2006-11890
    ```

    </details>

12. üïê **Invisible Backdoor Attacks Against Deep Neural Networks (2019 arXiv)**
    [[Notes](./notes/li2019invisible.md)]
    [[Paper](https://arxiv.org/pdf/1909.02742)]
    ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** xxx.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{li2019invisible,
        author    = {Shaofeng Li and
                    Benjamin Zi Hao Zhao and
                    Jiahao Yu and
                    Minhui Xue and
                    Dali Kaafar and
                    Haojin Zhu},
        title     = {Invisible Backdoor Attacks Against Deep Neural Networks},
        journal   = {CoRR},
        volume    = {abs/1909.02742},
        year      = {2019},
        url       = {http://arxiv.org/abs/1909.02742},
        archivePrefix = {arXiv},
        eprint    = {1909.02742},
        timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1909-02742.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1909.02742
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1909-02742
    ```

    </details>

## Defence

1.  ‚õîÔ∏è **Resilience of Pruned Neural Network Against Poisoning Attack (2018 MALWARE)**
    [[Notes](./notes/zhao2018resilience.md)]
    [[Paper](https://ieeexplore.ieee.org/abstract/document/8659362)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{zhao2018resilience,
        author    = {Bingyin Zhao and
                    Yingjie Lao},
        title     = {Resilience of Pruned Neural Network Against Poisoning Attack},
        booktitle = {13th International Conference on Malicious and Unwanted Software,
                    {MALWARE} 2018, Nantucket, MA, USA, October 22-24, 2018},
        pages     = {78--83},
        publisher = {{IEEE}},
        year      = {2018},
        url       = {https://doi.org/10.1109/MALWARE.2018.8659362},
        doi       = {10.1109/MALWARE.2018.8659362},
        timestamp = {Wed, 16 Oct 2019 14:14:55 +0200},
        biburl    = {https://dblp.org/rec/conf/malware/ZhaoL18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://ieeexplore.ieee.org/abstract/document/8659362
    Citation: https://dblp.org/rec/bibtex/conf/malware/ZhaoL18
    ```

    </details>

2.  ‚õîÔ∏è **Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks (2018 RAID)**
    [[Notes](./notes/liu2018finepruning.md)]
    [[Paper](https://arxiv.org/pdf/1805.12185)]
    ‚≠ê‚≠ê

    **Overview:** Pruning the network to remove the backdoor behaviors.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{liu2018finepruning,
        author    = {Kang Liu and
                    Brendan Dolan{-}Gavitt and
                    Siddharth Garg},
        editor    = {Michael Bailey and
                    Thorsten Holz and
                    Manolis Stamatogiannakis and
                    Sotiris Ioannidis},
        title     = {Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural
                    Networks},
        booktitle = {Research in Attacks, Intrusions, and Defenses - 21st International
                    Symposium, {RAID} 2018, Heraklion, Crete, Greece, September 10-12,
                    2018, Proceedings},
        series    = {Lecture Notes in Computer Science},
        volume    = {11050},
        pages     = {273--294},
        publisher = {Springer},
        year      = {2018},
        url       = {https://doi.org/10.1007/978-3-030-00470-5\_13},
        doi       = {10.1007/978-3-030-00470-5\_13},
        timestamp = {Wed, 25 Sep 2019 18:13:01 +0200},
        biburl    = {https://dblp.org/rec/conf/raid/0017DG18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1805.12185
    Citation: https://dblp.org/rec/bibtex/conf/raid/0017DG18
    ```

    </details>

3.  ‚õîÔ∏è **Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering (2019 AAAI)**
    [[Notes](./notes/chen2019ac.md)]
    [[Paper](https://arxiv.org/pdf/1811.03728)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Cluster analysis for each output label.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{chen2019ac,
        author    = {Bryant Chen and
                    Wilka Carvalho and
                    Nathalie Baracaldo and
                    Heiko Ludwig and
                    Benjamin Edwards and
                    Taesung Lee and
                    Ian Molloy and
                    Biplav Srivastava},
        editor    = {Hu{\'{a}}scar Espinoza and
                    Se{\'{a}}n {\'{O}} h{\'{E}}igeartaigh and
                    Xiaowei Huang and
                    Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo and
                    Mauricio Castillo{-}Effen},
        title     = {Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering},
        booktitle = {Workshop on Artificial Intelligence Safety 2019 co-located with the
                    Thirty-Third {AAAI} Conference on Artificial Intelligence 2019 (AAAI-19),
                    Honolulu, Hawaii, January 27, 2019},
        series    = {{CEUR} Workshop Proceedings},
        volume    = {2301},
        publisher = {CEUR-WS.org},
        year      = {2019},
        url       = {http://ceur-ws.org/Vol-2301/paper\_18.pdf},
        timestamp = {Thu, 05 Mar 2020 09:12:35 +0100},
        biburl    = {https://dblp.org/rec/conf/aaai/ChenCBLELMS19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1811.03728
    Citation: https://dblp.org/rec/bibtex/conf/aaai/ChenCBLELMS19
    ```

    </details>

4.  ‚õîÔ∏è **SentiNet: Detecting Physical Attacks Against Deep Learning Systems (2018 arXiv)**
    [[Notes](./notes/chou2018sentinet.md)]
    [[Paper](https://arxiv.org/pdf/1812.00292)]
    ‚≠ê‚≠ê‚≠ê

    **Overview:** Detect a potential attack region using Grad-CAM.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{chou2018sentinet,
        author    = {Edward Chou and
                    Florian Tram{\`{e}}r and
                    Giancarlo Pellegrino and
                    Dan Boneh},
        title     = {SentiNet: Detecting Physical Attacks Against Deep Learning Systems},
        journal   = {CoRR},
        volume    = {abs/1812.00292},
        year      = {2018},
        url       = {http://arxiv.org/abs/1812.00292},
        archivePrefix = {arXiv},
        eprint    = {1812.00292},
        timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1812-00292.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1812.00292
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1812-00292
    ```

    </details>

5.  ‚õîÔ∏è **STRIP: A Defense Against Trojan Attacks on Deep Neural Networks (2019 ACSAC)**
    [[Notes](./notes/gao2019strip.md)]
    [[Paper](https://arxiv.org/pdf/1902.06531)]
    [[Code](https://github.com/garrisongys/STRIP)]
    ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** The state-of-the-art runtime trojan inputs detection method using STRong Intentional Perturbation.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{gao2019strip,
        author    = {Yansong Gao and
                    Change Xu and
                    Derui Wang and
                    Shiping Chen and
                    Damith Chinthana Ranasinghe and
                    Surya Nepal},
        editor    = {David Balenson},
        title     = {{STRIP:} a defence against trojan attacks on deep neural networks},
        booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference,
                        {ACSAC} 2019, San Juan, PR, USA, December 09-13, 2019},
        pages     = {113--125},
        publisher = {{ACM}},
        year      = {2019},
        url       = {https://doi.org/10.1145/3359789.3359790},
        doi       = {10.1145/3359789.3359790},
        timestamp = {Tue, 26 Nov 2019 09:57:54 +0100},
        biburl    = {https://dblp.org/rec/conf/acsac/GaoXW0RN19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1902.06531
    Code: https://github.com/garrisongys/STRIP
    Citation: https://dblp.org/rec/bibtex/conf/acsac/GaoXW0RN19
    ```

    </details>

6.  ‚õîÔ∏è **NIC: Detecting Adversarial Samples with Neural Network Invariant Checking (2019 NDSS)**
    [[Notes](./notes/ma2019nic.md)]
    [[Paper](https://par.nsf.gov/servlets/purl/10139597)]
    [[Code](https://github.com/RU-System-Software-and-Security/NIC)]
    ‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** Inspired by software invariant checking.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{ma2019nic,
        author    = {Shiqing Ma and
                    Yingqi Liu and
                    Guanhong Tao and
                    Wen{-}Chuan Lee and
                    Xiangyu Zhang},
        title     = {{NIC:} Detecting Adversarial Samples with Neural Network Invariant
                    Checking},
        booktitle = {26th Annual Network and Distributed System Security Symposium, {NDSS}
                    2019, San Diego, California, USA, February 24-27, 2019},
        publisher = {The Internet Society},
        year      = {2019},
        url       = {https://www.ndss-symposium.org/ndss-paper/nic-detecting-adversarial-samples-with-neural-network-invariant-checking/},
        timestamp = {Thu, 02 May 2019 15:52:50 +0200},
        biburl    = {https://dblp.org/rec/conf/ndss/MaLTL019.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://par.nsf.gov/servlets/purl/10139597
    Code: https://github.com/RU-System-Software-and-Security/NIC
    Citation: https://dblp.org/rec/bibtex/conf/ndss/MaLTL019
    ```

    </details>

7.  ‚õîÔ∏è **Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks (2019 SP)**
    [[Notes](./notes/wang2019nc.md)]
    [[Paper](https://www.shawnshan.com/files/publication/backdoor-sp19.pdf)]
    [[Code](https://github.com/bolunwang/backdoor)]
    ‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** Finding the smallest perturbation pixel set, which can transform all classes to the target class.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{wang2019nc,
        author    = {Bolun Wang and
                    Yuanshun Yao and
                    Shawn Shan and
                    Huiying Li and
                    Bimal Viswanath and
                    Haitao Zheng and
                    Ben Y. Zhao},
        title     = {Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural
                    Networks},
        booktitle = {2019 {IEEE} Symposium on Security and Privacy, {SP} 2019, San Francisco,
                    CA, USA, May 19-23, 2019},
        pages     = {707--723},
        publisher = {{IEEE}},
        year      = {2019},
        url       = {https://doi.org/10.1109/SP.2019.00031},
        doi       = {10.1109/SP.2019.00031},
        timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
        biburl    = {https://dblp.org/rec/conf/sp/WangYSLVZZ19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://www.shawnshan.com/files/publication/backdoor-sp19.pdf
    Code: https://github.com/bolunwang/backdoor
    Citation: https://dblp.org/rec/bibtex/conf/sp/WangYSLVZZ19
    ```

    </details>

8.  ‚õîÔ∏è **DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks (2019 IJCAI)**
    [[Notes](./notes/chen2019deepinspect.md)]
    [[Paper](http://www.aceslab.org/sites/default/files/DeepInspect.pdf)]

    **Overview:** Train a cGAN to reconstruct the triggers.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{chen2019deepinspect,
        author    = {Huili Chen and
                    Cheng Fu and
                    Jishen Zhao and
                    Farinaz Koushanfar},
        editor    = {Sarit Kraus},
        title     = {DeepInspect: {A} Black-box Trojan Detection and Mitigation Framework
                    for Deep Neural Networks},
        booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
                    Artificial Intelligence, {IJCAI} 2019, Macao, China, August 10-16,
                    2019},
        pages     = {4658--4664},
        publisher = {ijcai.org},
        year      = {2019},
        url       = {https://doi.org/10.24963/ijcai.2019/647},
        doi       = {10.24963/ijcai.2019/647},
        timestamp = {Tue, 20 Aug 2019 16:18:18 +0200},
        biburl    = {https://dblp.org/rec/conf/ijcai/ChenFZK19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: http://www.aceslab.org/sites/default/files/DeepInspect.pdf
    Citation: https://dblp.org/rec/bibtex/conf/ijcai/ChenFZK19
    ```

    </details>

9.  ‚õîÔ∏è **ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (2019 CCS)**
    [[Notes](./notes/liu2019abs.md)]
    [[Paper](https://www.cs.rutgers.edu/~sm2283/papers/CCS19.pdf)]
    [[Code]()]
    ‚≠ê‚≠ê‚≠ê‚≠ê

    **Overview:** Stimulate the inner neural and analyze the output changes.

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{liu2019abs,
        author    = {Yingqi Liu and
                    Wen{-}Chuan Lee and
                    Guanhong Tao and
                    Shiqing Ma and
                    Yousra Aafer and
                    Xiangyu Zhang},
        editor    = {Lorenzo Cavallaro and
                    Johannes Kinder and
                    XiaoFeng Wang and
                    Jonathan Katz},
        title     = {{ABS:} Scanning Neural Networks for Back-doors by Artificial Brain
                    Stimulation},
        booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} Conference on Computer and
                    Communications Security, {CCS} 2019, London, UK, November 11-15, 2019},
        pages     = {1265--1282},
        publisher = {{ACM}},
        year      = {2019},
        url       = {https://doi.org/10.1145/3319535.3363216},
        doi       = {10.1145/3319535.3363216},
        timestamp = {Fri, 06 Mar 2020 13:19:56 +0100},
        biburl    = {https://dblp.org/rec/conf/ccs/LiuLTMAZ19.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://www.cs.rutgers.edu/~sm2283/papers/CCS19.pdf
    Code: ()
    Citation: https://dblp.org/rec/bibtex/conf/ccs/LiuLTMAZ19
    ```

    </details>

10. ‚õîÔ∏è **TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems (2019 arXiv)**
    [[Notes](./notes/guo2019tabor.md)]
    [[Paper](https://arxiv.org/pdf/1908.01763)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{guo2019tabor,
        author    = {Wenbo Guo and
                    Lun Wang and
                    Xinyu Xing and
                    Min Du and
                    Dawn Song},
        title     = {{TABOR:} {A} Highly Accurate Approach to Inspecting and Restoring
                    Trojan Backdoors in {AI} Systems},
        journal   = {CoRR},
        volume    = {abs/1908.01763},
        year      = {2019},
        url       = {http://arxiv.org/abs/1908.01763},
        archivePrefix = {arXiv},
        eprint    = {1908.01763},
        timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1908-01763.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1908.01763
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1908-01763
    ```

    </details>

11. ‚õîÔ∏è **Detecting AI Trojans Using Meta Neural Analysis (2019 arXiv)**
    [[Notes](./notes/xu2019metaneural.md)]
    [[Paper](https://arxiv.org/pdf/1910.03137)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{xu2019metaneural,
        author    = {Xiaojun Xu and
                    Qi Wang and
                    Huichen Li and
                    Nikita Borisov and
                    Carl A. Gunter and
                    Bo Li},
        title     = {Detecting {AI} Trojans Using Meta Neural Analysis},
        journal   = {CoRR},
        volume    = {abs/1910.03137},
        year      = {2019},
        url       = {http://arxiv.org/abs/1910.03137},
        archivePrefix = {arXiv},
        eprint    = {1910.03137},
        timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03137.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1910.03137
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1910-03137
    ```

    </details>

12. ‚õîÔ∏è **Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification (2019 arXiv)**
    [[Notes](./notes/xiang2019revealing.md)]
    [[Paper](https://arxiv.org/pdf/1908.10498)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{xiang2019revealing,
        author    = {Zhen Xiang and
                    David J. Miller and
                    George Kesidis},
        title     = {Revealing Backdoors, Post-Training, in {DNN} Classifiers via Novel
                    Inference on Optimized Perturbations Inducing Group Misclassification},
        journal   = {CoRR},
        volume    = {abs/1908.10498},
        year      = {2019},
        url       = {http://arxiv.org/abs/1908.10498},
        archivePrefix = {arXiv},
        eprint    = {1908.10498},
        timestamp = {Fri, 30 Aug 2019 08:41:20 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10498.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1908.10498
    Citation: https://dblp.org/rec/bibtex/journals/corr/abs-1908-10498
    ```

    </details>

## Usage

1.  ‚õîÔ∏è **Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring (2018 USENIX)**
    [[Notes](./notes/adi2018turning.md)]
    [[Paper](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-adi.pdf)]
    ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{adi2018turning,
        author    = {Yossi Adi and
                    Carsten Baum and
                    Moustapha Ciss{\'{e}} and
                    Benny Pinkas and
                    Joseph Keshet},
        editor    = {William Enck and
                    Adrienne Porter Felt},
        title     = {Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks
                    by Backdooring},
        booktitle = {27th {USENIX} Security Symposium, {USENIX} Security 2018, Baltimore,
                    MD, USA, August 15-17, 2018},
        pages     = {1615--1631},
        publisher = {{USENIX} Association},
        year      = {2018},
        url       = {https://www.usenix.org/conference/usenixsecurity18/presentation/adi},
        timestamp = {Mon, 20 Aug 2018 15:16:57 +0200},
        biburl    = {https://dblp.org/rec/conf/uss/AdiBCPK18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-adi.pdf
    Citation: https://dblp.org/rec/bibtex/conf/uss/AdiBCPK18
    ```

    </details>

2.  ‚õîÔ∏è **Watermarking Deep Neural Networks for Embedded Systems (2018 ICCAD)**
    [[Notes](./notes/guo2018watermarking.md)]
    [[Paper](http://web.cs.ucla.edu/~miodrag/papers/Guo_ICCAD_2018.pdf)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @inproceedings{guo2018watermarking,
        author    = {Jia Guo and
                    Miodrag Potkonjak},
        editor    = {Iris Bahar},
        title     = {Watermarking Deep Neural Networks for Embedded Systems},
        booktitle = {Proceedings of the International Conference on Computer-Aided Design,
                    {ICCAD} 2018, San Diego, CA, USA, November 05-08, 2018},
        pages     = {133},
        publisher = {{ACM}},
        year      = {2018},
        url       = {https://doi.org/10.1145/3240765.3240862},
        doi       = {10.1145/3240765.3240862},
        timestamp = {Mon, 07 Jan 2019 11:16:31 +0100},
        biburl    = {https://dblp.org/rec/conf/iccad/GuoP18.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
    ```

    #### URL

    ```
    Paper: http://web.cs.ucla.edu/~miodrag/papers/Guo_ICCAD_2018.pdf
    Citation: https://dblp.org/rec/bibtex/conf/iccad/GuoP18
    ```

    </details>

3.  ‚õîÔ∏è **Using Honeypots to Catch Adversarial Attacks on Neural Networks (2019 arXiv)**
    [[Notes](./notes/shan2019honeypots.md)]
    [[Paper](https://arxiv.org/pdf/1904.08554)]

    <details>
    <summary>Details (click to expand...)</summary>

    #### Citation

    ```
    @article{shan2019honeypots,
        author    = {Shawn Shan and
                    Emily Wenger and
                    Bolun Wang and
                    Bo Li and
                    Haitao Zheng and
                    Ben Y. Zhao},
        title     = {Using Honeypots to Catch Adversarial Attacks on Neural Networks},
        journal   = {CoRR},
        volume    = {abs/1904.08554},
        year      = {2019},
        url       = {https://arxiv.org/abs/1904.08554},
        archivePrefix = {arXiv},
        eprint    = {1904.08554}
    }
    ```

    #### URL

    ```
    Paper: https://arxiv.org/pdf/1904.08554
    Citation: https://arxiv.org/abs/1904.08554
    ```

    </details>
